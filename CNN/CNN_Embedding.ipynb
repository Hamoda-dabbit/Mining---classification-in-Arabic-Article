{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamoda-dabbit/Mining---classification-in-Arabic-Article/blob/main/CNN/CNN_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification by Convolutional Neural Network and word Embedding**\n",
        "\n",
        "----------"
      ],
      "metadata": {
        "id": "lM5DgwrrVpeO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKSuILzq_b-8"
      },
      "source": [
        " import library\n",
        "\n",
        "  إستيراد المكتبات"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "import re\n",
        "import string\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# fix random seed for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYp37W3G3c-N",
        "outputId": "7e8aafe0-7b6e-4ad4-c451-bb16d17d2c78"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY_2pUn__oh8"
      },
      "source": [
        "--------\n",
        "\n",
        "**1- Import Data set**\n",
        "\n",
        "استيراد ملفات البيانات\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITaTjKYUJGKx",
        "outputId": "53c8273e-442a-4c85-871b-bc51c9600e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 68690 entries, 0 to 68689\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    68312 non-null  object\n",
            " 1   targe   68690 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 1.0+ MB\n"
          ]
        }
      ],
      "source": [
        "# dataset = pd.read_excel(\"drive/MyDrive/mining/1.xlsx\")\n",
        "# dataset = pd.read_excel(\"drive/MyDrive/mining/2.xlsx\")\n",
        "# dataset = pd.read_excel(\"drive/MyDrive/mining/3.xlsx\")\n",
        "dataset = pd.read_excel(\"drive/MyDrive/mining/4.xlsx\")\n",
        "\n",
        "dataset.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjXeZSy9_fB2"
      },
      "source": [
        "------\n",
        "shuffle  \n",
        "بعثرة البيانات"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "n70SDzKWdM5_",
        "outputId": "9a41f9fd-f71f-411a-f4cb-a834ed0d3952"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text  targe\n",
              "18590  بجمال فريد وشخصية قوية متعددة المواهب استطاعت ...      0\n",
              "27642  أخبارنا المغربية متابعة حضر ثلاثة مستشارين فقط...      3\n",
              "7875   عبدالاله بوسحابة اخبارنا المغربية تعقد الهيئة ...      2\n",
              "66249  في ما يلي نتائج عملية سحب قرعة نهائيات كأس الع...      4\n",
              "36467  ندد عدد من الفنانين الإيطاليين والمغاربة بطريق...      0\n",
              "...                                                  ...    ...\n",
              "37194  تمكنت عناصر الدرك الملكي من توقيف مرتكب جريمة ...      1\n",
              "6265   تمكنت عناصر الجمارك المشتغلة بالمعبر الحدودي ب...      1\n",
              "54886  اهتزت القنيطرة نهاية الأسبوع الماضي على وقع فض...      1\n",
              "860    أخبارنا فهـــد الباهـــي عاد المدون والبودكاس ...      0\n",
              "15795  تبدأ يوم الثلاثاء يونيو حزيران وقائع محاكمة لا...      4\n",
              "\n",
              "[68690 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be35af94-7f77-4747-b429-117f1e4cc3d4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>targe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18590</th>\n",
              "      <td>بجمال فريد وشخصية قوية متعددة المواهب استطاعت ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27642</th>\n",
              "      <td>أخبارنا المغربية متابعة حضر ثلاثة مستشارين فقط...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7875</th>\n",
              "      <td>عبدالاله بوسحابة اخبارنا المغربية تعقد الهيئة ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66249</th>\n",
              "      <td>في ما يلي نتائج عملية سحب قرعة نهائيات كأس الع...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36467</th>\n",
              "      <td>ندد عدد من الفنانين الإيطاليين والمغاربة بطريق...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37194</th>\n",
              "      <td>تمكنت عناصر الدرك الملكي من توقيف مرتكب جريمة ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6265</th>\n",
              "      <td>تمكنت عناصر الجمارك المشتغلة بالمعبر الحدودي ب...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54886</th>\n",
              "      <td>اهتزت القنيطرة نهاية الأسبوع الماضي على وقع فض...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>أخبارنا فهـــد الباهـــي عاد المدون والبودكاس ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15795</th>\n",
              "      <td>تبدأ يوم الثلاثاء يونيو حزيران وقائع محاكمة لا...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68690 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be35af94-7f77-4747-b429-117f1e4cc3d4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-be35af94-7f77-4747-b429-117f1e4cc3d4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-be35af94-7f77-4747-b429-117f1e4cc3d4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "dataset=shuffle(dataset)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWomJ3tI_qIQ"
      },
      "source": [
        "------\n",
        "**2- Clean Data**\n",
        "\n",
        "تنظيف البيانات"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFsW7x2E1vbw",
        "outputId": "92cdb2bd-2985-43c8-e41f-82119414f5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "378\n",
            "0\n",
            "                                                    text  targe\n",
            "18590  بجمال فريد وشخصيه قويه متعده المواهب استطاعت ا...      0\n",
            "27642  اخبارنا المغربيه متابعه حضر ثلاثه مستشارين فقط...      3\n",
            "7875   عبدالاله بوسحابه اخبارنا المغربيه تعقد الهيئه ...      2\n",
            "66249  في ما يلي نتائج عمليه سحب قرعه نهائيات كاس الع...      4\n",
            "36467  ند عد من الفنانين الايطالين والمغاربه بطريقه ا...      0\n",
            "...                                                  ...    ...\n",
            "37194  تمكنت عناصر الدرك الملكي من توقيف مرتكب جريمه ...      1\n",
            "6265   تمكنت عناصر الجمارك المشتغله بالمعبر الحدودي ب...      1\n",
            "54886  اهتزت القنيطره نهايه الاسبوع الماضي علي وقع فض...      1\n",
            "860    اخبارنا فهد الباهي عاد المدون والبودكاس المغرب...      0\n",
            "15795  تبدا يوم الثلاثاء يونيو حزيران وقائع محاكمه لا...      4\n",
            "\n",
            "[68312 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print(np.sum(dataset.isnull().any(axis=1)))\n",
        "dataset= dataset.dropna()\n",
        "print(np.sum(dataset.isnull().any(axis=1)))\n",
        "\n",
        "#-------------\n",
        "def remove_hashtag(df, col = 'text'):\n",
        "    for letter in r'#.][!XR':\n",
        "      df[col] = df[col].astype(str).str.replace(letter,'', regex=True)\n",
        "           \n",
        "remove_hashtag(dataset)\n",
        "#-------------\n",
        "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
        "english_punctuations = string.punctuation\n",
        "punctuations_list = arabic_punctuations + english_punctuations\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)\n",
        "#-------------\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text\n",
        "#-------------    \n",
        "def remove_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "#-------------\n",
        "def processDocument(doc, stemmer): \n",
        "\n",
        "    #Replace @username with empty string\n",
        "    doc = re.sub(r'@[^\\s]+', ' ', doc)\n",
        "    doc = re.sub(r'_', ' ', doc)\n",
        "    doc = re.sub(r'\\n', ' ', doc)\n",
        "    doc = re.sub(r'[a-z,A-Z]', '', doc)\n",
        "    doc = re.sub(r'\\d', '', doc)\n",
        "    #Convert www.* or https?://* to \" \"\n",
        "    doc = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',doc)\n",
        "    #Replace #word with word\n",
        "    doc = re.sub(r'#([^\\s]+)', r'\\1', doc)\n",
        "    # remove punctuations\n",
        "    doc= remove_punctuations(doc)\n",
        "    # normalize the tweet\n",
        "    doc= normalize_arabic(doc)\n",
        "    # remove repeated letters\n",
        "    doc=remove_repeating_char(doc)\n",
        "    #stemming\n",
        "    doc = stemmer.stem(doc)\n",
        "    \n",
        "    return doc\n",
        "    \n",
        "stemmer = ISRIStemmer()\n",
        "dataset[\"text\"] = dataset['text'].apply(lambda x: processDocument(x, stemmer))\n",
        "print(dataset)\n",
        "\n",
        "reviews = dataset['text'].values\n",
        "t = Tokenizer(oov_token='<UNK>')\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(reviews)\n",
        "t.word_index['<PAD>'] = 0\n",
        "\n",
        "VOCAB_SIZE = len(t.word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------\n",
        "Tokenizer function\n"
      ],
      "metadata": {
        "id": "i_2WrVaGWfkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Tokenizer_UNK(train_x,test_reviews,train_y, test_y):\n",
        "  norm_train_reviews = (train_x)\n",
        "  norm_test_reviews = (test_reviews)\n",
        "\n",
        "  le = LabelEncoder()\n",
        "  y_train = le.fit_transform(train_y)\n",
        "  y_test = le.transform(test_y)\n",
        "\n",
        "  from keras.utils.np_utils import to_categorical\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "  train_sequences = t.texts_to_sequences(norm_train_reviews)\n",
        "  test_sequences = t.texts_to_sequences(norm_test_reviews)\n",
        "  print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
        "  print(\"Number of Documents={}\".format(t.document_count))\n",
        "\n",
        "  MAX_SEQUENCE_LENGTH = 1000\n",
        "  # pad dataset to a maximum review length in words\n",
        "  X_train = sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  X_test = sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  # X_train.shape, X_test.shape\n",
        "  return X_train, X_test ,y_train,y_test"
      ],
      "metadata": {
        "id": "lZ_oeP-UuRiQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------\n",
        "model CNN function\n",
        "\n",
        "عمل تابع لبناء موديل الشبكة العصبية العميقة"
      ],
      "metadata": {
        "id": "zXpuxqSuWY_Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AXhAERVeXhmZ"
      },
      "outputs": [],
      "source": [
        "def set_model():\n",
        "  VOCAB_SIZE = len(t.word_index)\n",
        "  MAX_SEQUENCE_LENGTH = 1000\n",
        "  EMBED_SIZE = 300\n",
        "  \n",
        "  # create the model\n",
        "  global model\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=MAX_SEQUENCE_LENGTH))\n",
        "  model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "\n",
        "  # model.add(Dense(1, activation='sigmoid'))\n",
        "  model.add(Dense(5, activation='sigmoid'))\n",
        "  # model.add(Flatten())\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------\n",
        "accuracy calculation function\n",
        "\n",
        "عمل تابع لحساب الدقة"
      ],
      "metadata": {
        "id": "CBw7IYEkWWD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zm-pc34lkk25"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "def calculate_results(y_true, y_pred):\n",
        "  # Calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                  \"precision\": model_precision,\n",
        "                  \"recall\": model_recall,\n",
        "                  \"f1\": model_f1}\n",
        "  return model_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------\n",
        "**3- Feature Extraction:**  Word Embedding\n",
        "\n",
        "استخراج الميزات(تحويل النص إلى أرقام)\n",
        "\n",
        "\n",
        "**4- classification:** CNN\n",
        "\n",
        "تصنيف البيانات\n",
        "\n",
        "with Cross validation\n",
        "\n",
        "لتوزيع عينات الاختبار والتأكد من عدم وجود\n",
        "\n",
        "Overfitting"
      ],
      "metadata": {
        "id": "Lsfshy_jWzs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# build train and test datasets\n",
        "reviews = dataset['text'].values\n",
        "sentiments = dataset['targe'].values\n",
        "#####\n",
        "strtfdKFold = StratifiedKFold(n_splits=4)#5\n",
        "kfold = strtfdKFold.split(reviews, sentiments)\n",
        "result_CNNClassifier=[]\n",
        "#\n",
        "for k, (train_index, test_index) in enumerate(kfold):\n",
        "    # if(os.path.exists(\"drive/MyDrive/mining/CNN.net\")):\n",
        "    #   print(\"load net\")\n",
        "    #   model = keras.models.load_model('drive/MyDrive/mining/CNN.net') \n",
        "\n",
        "    X_train_, X_test_ = reviews[train_index], reviews[test_index]\n",
        "    y_train_, y_test_ = sentiments[train_index], sentiments[test_index]\n",
        "\n",
        "    X_train, X_test ,y_train,y_test=Tokenizer_UNK(X_train_,X_test_,y_train_,y_test_)\n",
        "    # set_model()\n",
        "\n",
        "    # Fit the model\n",
        "    model=set_model()\n",
        "    EPOCHS=2  \n",
        "    BATCH_SIZE=128\n",
        "    model.fit(X_train, y_train, validation_split=0.1, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
        "    y_pred=(model.predict(X_test))\n",
        "    y_classes_pre = [np.argmax(y, axis=None, out=None) for y in y_pred]\n",
        "    y_classes_test = [np.argmax(y, axis=None, out=None) for y in y_test]\n",
        "    result_CNNClassifier.append(calculate_results(y_classes_test, y_classes_pre))\n",
        "    # model.save('drive/MyDrive/mining/CNN.net')\n",
        "    # print(\"save net\")\n",
        "\n",
        "result_CNNClassifier"
      ],
      "metadata": {
        "id": "e_0JxFY4oRce",
        "outputId": "91af8380-85b0-4e64-a238-cf5c6c1629da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size=325418\n",
            "Number of Documents=68312\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1000, 300)         97625400  \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 1000, 128)         153728    \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 500, 128)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 500, 64)           32832     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 250, 64)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 250, 32)           8224      \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 125, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4000)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               1024256   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 98,845,725\n",
            "Trainable params: 98,845,725\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "361/361 [==============================] - 1375s 4s/step - loss: 0.1551 - accuracy: 0.8234 - val_loss: 0.0691 - val_accuracy: 0.9407\n",
            "Epoch 2/2\n",
            "361/361 [==============================] - 1371s 4s/step - loss: 0.0324 - accuracy: 0.9758 - val_loss: 0.0736 - val_accuracy: 0.9393\n",
            "Vocabulary size=325418\n",
            "Number of Documents=68312\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 1000, 300)         97625400  \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 1000, 128)         153728    \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 500, 128)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 500, 64)           32832     \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPooling  (None, 250, 64)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 250, 32)           8224      \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPooling  (None, 125, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4000)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               1024256   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 98,845,725\n",
            "Trainable params: 98,845,725\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "361/361 [==============================] - 1361s 4s/step - loss: 0.1464 - accuracy: 0.8362 - val_loss: 0.0661 - val_accuracy: 0.9459\n",
            "Epoch 2/2\n",
            "361/361 [==============================] - 1369s 4s/step - loss: 0.0306 - accuracy: 0.9763 - val_loss: 0.0815 - val_accuracy: 0.9354\n",
            "Vocabulary size=325418\n",
            "Number of Documents=68312\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 1000, 300)         97625400  \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 1000, 128)         153728    \n",
            "                                                                 \n",
            " max_pooling1d_6 (MaxPooling  (None, 500, 128)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 500, 64)           32832     \n",
            "                                                                 \n",
            " max_pooling1d_7 (MaxPooling  (None, 250, 64)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 250, 32)           8224      \n",
            "                                                                 \n",
            " max_pooling1d_8 (MaxPooling  (None, 125, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 4000)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               1024256   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 98,845,725\n",
            "Trainable params: 98,845,725\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "361/361 [==============================] - 1383s 4s/step - loss: 0.1676 - accuracy: 0.8021 - val_loss: 0.0712 - val_accuracy: 0.9395\n",
            "Epoch 2/2\n",
            "361/361 [==============================] - 1381s 4s/step - loss: 0.0321 - accuracy: 0.9745 - val_loss: 0.0725 - val_accuracy: 0.9381\n",
            "Vocabulary size=325418\n",
            "Number of Documents=68312\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 1000, 300)         97625400  \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 1000, 128)         153728    \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPooling  (None, 500, 128)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 500, 64)           32832     \n",
            "                                                                 \n",
            " max_pooling1d_10 (MaxPoolin  (None, 250, 64)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 250, 32)           8224      \n",
            "                                                                 \n",
            " max_pooling1d_11 (MaxPoolin  (None, 125, 32)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 4000)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               1024256   \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 98,845,725\n",
            "Trainable params: 98,845,725\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "361/361 [==============================] - 1406s 4s/step - loss: 0.1569 - accuracy: 0.8200 - val_loss: 0.0718 - val_accuracy: 0.9350\n",
            "Epoch 2/2\n",
            "361/361 [==============================] - 1401s 4s/step - loss: 0.0330 - accuracy: 0.9743 - val_loss: 0.0651 - val_accuracy: 0.9442\n",
            "CPU times: user 5h 56min 21s, sys: 6min 48s, total: 6h 3min 10s\n",
            "Wall time: 3h 14min 34s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "حساب متوسط النتائج للـ5 مراحل من\n",
        "\n",
        " cross validation "
      ],
      "metadata": {
        "id": "In5dWzCIXIyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy=0\n",
        "f1=0\n",
        "precision=0\n",
        "recall=0\n",
        "for i in result_CNNClassifier:\n",
        "  accuracy+=(i['accuracy'])\n",
        "  f1+=(i['f1'])\n",
        "  precision+=(i['precision'])\n",
        "  recall+=(i['recall'])\n",
        "\n",
        "accuracy=accuracy/len(result_CNNClassifier)\n",
        "f1=f1/len(result_CNNClassifier)\n",
        "precision=precision/len(result_CNNClassifier)\n",
        "recall=recall/len(result_CNNClassifier)\n",
        "# (result_DecisionTreeClassifier)\n",
        "results_CNNClassifier=[{'accuracy':accuracy,'f1':f1,'precision':precision,'recall':recall}]\n",
        "results_CNNClassifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDPaXpSn0p-S",
        "outputId": "e9156441-88a0-478f-9fc6-89be2e51ee2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'accuracy': 94.34652769645157,\n",
              "  'f1': 0.9436543950194775,\n",
              "  'precision': 0.9441542400003347,\n",
              "  'recall': 0.9434652769645158}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "-----\n",
        "---------\n",
        "**5- Show Results**\n",
        "\n",
        "النتائج\n",
        "\n",
        "نتائج التدريب لكل قاعدة بيانات"
      ],
      "metadata": {
        "id": "gXWU0K4mXLVq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-5UX4r2kr7e",
        "outputId": "331785c6-9367-4029-b7d8-06ed4c2153d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNClassifier results in dataset 1:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'accuracy': 95.19608342701729,\n",
              "  'f1': 0.9520984811390116,\n",
              "  'precision': 0.952799253715478,\n",
              "  'recall': 0.9519608342701729}]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "print('CNNClassifier results in dataset 1:')\n",
        "results_CNNClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yU3FWF6a0_9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ac5ce3-b2d1-4b5c-b085-4fad6d8eb0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNClassifier results in dataset 2:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'accuracy': 95.57787784320516,\n",
              "  'f1': 0.9558535748300416,\n",
              "  'precision': 0.9565019386666695,\n",
              "  'recall': 0.9557787784320517}]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "print('CNNClassifier results in dataset 2:')\n",
        "results_CNNClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvnBdsMv1AHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d22ea8-203c-457d-8f62-8e4191151801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNClassifier results in dataset 3:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'accuracy': 94.5945934848568,\n",
              "  'f1': 0.9460234167421608,\n",
              "  'precision': 0.9465212822533673,\n",
              "  'recall': 0.9459459348485679}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "print('CNNClassifier results in dataset 3:')\n",
        "results_CNNClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4AvEaX51ASP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20574d39-9379-4a0d-beff-40a9e9758739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNClassifier results in dataset 4:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'accuracy': 94.78257724360446,\n",
              "  'f1': 0.9477387119920051,\n",
              "  'precision': 0.9479345864832764,\n",
              "  'recall': 0.9478257724360446}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "print('CNNClassifier results in dataset 4:')\n",
        "results_CNNClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Final_Result=[\n",
        "  {'accuracy': 95.19608342701729,\n",
        "  'f1': 0.9520984811390116,\n",
        "  'precision': 0.952799253715478,\n",
        "  'recall': 0.9519608342701729}\n",
        "  ,\n",
        "  {'accuracy': 95.57787784320516,\n",
        "  'f1': 0.9558535748300416,\n",
        "  'precision': 0.9565019386666695,\n",
        "  'recall': 0.9557787784320517}\n",
        "  ,\n",
        "  {'accuracy': 94.5945934848568,\n",
        "  'f1': 0.9460234167421608,\n",
        "  'precision': 0.9465212822533673,\n",
        "  'recall': 0.9459459348485679}\n",
        "  ,\n",
        " {'accuracy': 94.78257724360446,\n",
        "  'f1': 0.9477387119920051,\n",
        "  'precision': 0.9479345864832764,\n",
        "  'recall': 0.9478257724360446}\n",
        "  ]\n",
        "\n",
        "accuracy=0\n",
        "f1=0\n",
        "precision=0\n",
        "recall=0\n",
        "for i in Final_Result:\n",
        "  accuracy+=(i['accuracy'])\n",
        "  f1+=(i['f1'])\n",
        "  precision+=(i['precision'])\n",
        "  recall+=(i['recall'])\n",
        "\n",
        "accuracy=accuracy/len(Final_Result)\n",
        "f1=f1/len(Final_Result)\n",
        "precision=precision/len(Final_Result)\n",
        "recall=recall/len(Final_Result)\n",
        "# (result_DecisionTreeClassifier)\n",
        "Final_Result=[{'accuracy':accuracy,'f1':f1,'precision':precision,'recall':recall}]\n",
        "print(\"RAM: 1.6 , CPU times: user 5h 56min 21s \")\n",
        "print('Final_Result:')\n",
        "(Final_Result)"
      ],
      "metadata": {
        "id": "-lB0leWu1IwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c97aad-ce86-452d-863e-2d609698b741"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAM: 1.6 , CPU times: user 5h 56min 21s \n",
            "Final_Result:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'accuracy': 95.03778299967092,\n",
              "  'f1': 0.9504285461758049,\n",
              "  'precision': 0.9509392652796977,\n",
              "  'recall': 0.9503778299967094}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_qyQzgTNUlO"
      },
      "source": [
        "---------------\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CNN_Embedding.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}