{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamoda-dabbit/Mining---classification-in-Arabic-Article/blob/main/CNN/CNN_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification by Convolutional Neural Network and word Embedding**\n",
        "\n",
        "----------"
      ],
      "metadata": {
        "id": "lM5DgwrrVpeO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKSuILzq_b-8"
      },
      "source": [
        " import library\n",
        "\n",
        "  إستيراد المكتبات"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "import re\n",
        "import string\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# fix random seed for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYp37W3G3c-N",
        "outputId": "1e42bcf1-82e2-43bc-f514-88ecad94f004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY_2pUn__oh8"
      },
      "source": [
        "--------\n",
        "\n",
        "**1- Import Data set**\n",
        "\n",
        "استيراد ملفات البيانات\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITaTjKYUJGKx",
        "outputId": "eac8c8d2-3996-43d7-e22e-fefc59c5c7c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17500 entries, 0 to 17499\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    17361 non-null  object\n",
            " 1   targe   17500 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 273.6+ KB\n"
          ]
        }
      ],
      "source": [
        "dataset = pd.read_excel(\"drive/MyDrive/mining/1.xlsx\")\n",
        "# dataset = pd.read_excel(\"drive/MyDrive/mining/2.xlsx\")\n",
        "# dataset = pd.read_excel(\"drive/MyDrive/mining/3.xlsx\")\n",
        "# dataset = pd.read_excel(\"drive/MyDrive/mining/4.xlsx\")\n",
        "\n",
        "dataset.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjXeZSy9_fB2"
      },
      "source": [
        "------\n",
        "shuffle  \n",
        "بعثرة البيانات"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "n70SDzKWdM5_",
        "outputId": "a92b28d2-ec06-4383-e299-0277900b41b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text  targe\n",
              "1411   علمت الصباح أن الفيلم الجديد للمخرج نبيل عيوش ...      0\n",
              "7555   يستعد المغرب لإطلاق أول دفعة من السندات الإسلا...      2\n",
              "4930   استقلال القاضي هدف ثابت وحتمي لتحقيق العدالة ف...      1\n",
              "10054  بلغ حجم الإعفاءات الضريبية خلال العام الحالي م...      2\n",
              "2714   أخبارنا المغربية سناء الوردي استغلت الفنانة ال...      0\n",
              "...                                                  ...    ...\n",
              "11284  اختتمت شبيبة حزب التجمع الوطني للأحرار أشغال ج...      3\n",
              "11964  عب رت الفعاليات الثقافية والفكرية الملتئمة ضمن...      3\n",
              "5390   اعترضوه بالمدينة العتيقة ومارسوا عليه الجنس بع...      1\n",
              "860    تستعد لتقديم برنامجين تلفزيونيين جديدين بعد رم...      0\n",
              "15795  أغلق أمس الثلاثاء سوق الانتقالات الصيفية بصفقا...      4\n",
              "\n",
              "[17500 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dcc17c88-8f58-4bd6-858f-f7e8e3926447\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>targe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1411</th>\n",
              "      <td>علمت الصباح أن الفيلم الجديد للمخرج نبيل عيوش ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7555</th>\n",
              "      <td>يستعد المغرب لإطلاق أول دفعة من السندات الإسلا...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4930</th>\n",
              "      <td>استقلال القاضي هدف ثابت وحتمي لتحقيق العدالة ف...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10054</th>\n",
              "      <td>بلغ حجم الإعفاءات الضريبية خلال العام الحالي م...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2714</th>\n",
              "      <td>أخبارنا المغربية سناء الوردي استغلت الفنانة ال...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11284</th>\n",
              "      <td>اختتمت شبيبة حزب التجمع الوطني للأحرار أشغال ج...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11964</th>\n",
              "      <td>عب رت الفعاليات الثقافية والفكرية الملتئمة ضمن...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>اعترضوه بالمدينة العتيقة ومارسوا عليه الجنس بع...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>تستعد لتقديم برنامجين تلفزيونيين جديدين بعد رم...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15795</th>\n",
              "      <td>أغلق أمس الثلاثاء سوق الانتقالات الصيفية بصفقا...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17500 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dcc17c88-8f58-4bd6-858f-f7e8e3926447')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dcc17c88-8f58-4bd6-858f-f7e8e3926447 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dcc17c88-8f58-4bd6-858f-f7e8e3926447');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dataset=shuffle(dataset)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWomJ3tI_qIQ"
      },
      "source": [
        "------\n",
        "**2- Clean Data**\n",
        "\n",
        "تنظيف البيانات"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFsW7x2E1vbw",
        "outputId": "9ab459e0-3e85-47f7-8a75-7af6cde1ec41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "139\n",
            "0\n",
            "                                                    text  targe\n",
            "1411   علمت الصباح ان الفيلم الجديد لمخرج نبيل عيوش س...      0\n",
            "7555   يستعد المغرب لاطلاق اول دفعه من السندات الاسلا...      2\n",
            "4930   استقلال القاضي هدف ثابت وحتمي لتحقيق العداله ف...      1\n",
            "10054  بلغ حجم الاعفاءات الضريبيه خلال العام الحالي م...      2\n",
            "2714   اخبارنا المغربيه سناء الوردي استغلت الفنانه ال...      0\n",
            "...                                                  ...    ...\n",
            "11284  اختمت شبيبه حزب التجمع الوطني لاحرار اشغال جام...      3\n",
            "11964  عب رت الفعاليات الثقافيه والفكريه الملتئمه ضمن...      3\n",
            "5390   اعترضوه بالمدينه العتيقه ومارسوا عليه الجنس بع...      1\n",
            "860    تستعد لتقديم برنامجين تلفزيونين جديدين بعد رمض...      0\n",
            "15795  اغلق امس الثلاثاء سوق الانتقالات الصيفيه بصفقا...      4\n",
            "\n",
            "[17361 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print(np.sum(dataset.isnull().any(axis=1)))\n",
        "dataset= dataset.dropna()\n",
        "print(np.sum(dataset.isnull().any(axis=1)))\n",
        "\n",
        "#-------------\n",
        "def remove_hashtag(df, col = 'text'):\n",
        "    for letter in r'#.][!XR':\n",
        "      df[col] = df[col].astype(str).str.replace(letter,'', regex=True)\n",
        "           \n",
        "remove_hashtag(dataset)\n",
        "#-------------\n",
        "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
        "english_punctuations = string.punctuation\n",
        "punctuations_list = arabic_punctuations + english_punctuations\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)\n",
        "#-------------\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text\n",
        "#-------------    \n",
        "def remove_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "#-------------\n",
        "def processDocument(doc, stemmer): \n",
        "\n",
        "    #Replace @username with empty string\n",
        "    doc = re.sub(r'@[^\\s]+', ' ', doc)\n",
        "    doc = re.sub(r'_', ' ', doc)\n",
        "    doc = re.sub(r'\\n', ' ', doc)\n",
        "    doc = re.sub(r'[a-z,A-Z]', '', doc)\n",
        "    doc = re.sub(r'\\d', '', doc)\n",
        "    #Convert www.* or https?://* to \" \"\n",
        "    doc = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',doc)\n",
        "    #Replace #word with word\n",
        "    doc = re.sub(r'#([^\\s]+)', r'\\1', doc)\n",
        "    # remove punctuations\n",
        "    doc= remove_punctuations(doc)\n",
        "    # normalize the tweet\n",
        "    doc= normalize_arabic(doc)\n",
        "    # remove repeated letters\n",
        "    doc=remove_repeating_char(doc)\n",
        "    #stemming\n",
        "    doc = stemmer.stem(doc)\n",
        "    \n",
        "    return doc\n",
        "    \n",
        "stemmer = ISRIStemmer()\n",
        "dataset[\"text\"] = dataset['text'].apply(lambda x: processDocument(x, stemmer))\n",
        "print(dataset)\n",
        "\n",
        "reviews = dataset['text'].values\n",
        "t = Tokenizer(oov_token='<UNK>')\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(reviews)\n",
        "t.word_index['<PAD>'] = 0\n",
        "\n",
        "VOCAB_SIZE = len(t.word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------\n",
        "Tokenizer function\n"
      ],
      "metadata": {
        "id": "i_2WrVaGWfkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Tokenizer_UNK(train_x,test_reviews,train_y, test_y):\n",
        "  norm_train_reviews = (train_x)\n",
        "  norm_test_reviews = (test_reviews)\n",
        "\n",
        "  le = LabelEncoder()\n",
        "  y_train = le.fit_transform(train_y)\n",
        "  y_test = le.transform(test_y)\n",
        "\n",
        "  from keras.utils.np_utils import to_categorical\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "  train_sequences = t.texts_to_sequences(norm_train_reviews)\n",
        "  test_sequences = t.texts_to_sequences(norm_test_reviews)\n",
        "  print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
        "  print(\"Number of Documents={}\".format(t.document_count))\n",
        "\n",
        "  MAX_SEQUENCE_LENGTH = 1000\n",
        "  # pad dataset to a maximum review length in words\n",
        "  X_train = sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  X_test = sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  # X_train.shape, X_test.shape\n",
        "  return X_train, X_test ,y_train,y_test"
      ],
      "metadata": {
        "id": "lZ_oeP-UuRiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------\n",
        "model CNN function\n",
        "\n",
        "عمل تابع لبناء موديل الشبكة العصبية العميقة"
      ],
      "metadata": {
        "id": "zXpuxqSuWY_Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXhAERVeXhmZ"
      },
      "outputs": [],
      "source": [
        "def set_model():\n",
        "  VOCAB_SIZE = len(t.word_index)\n",
        "  MAX_SEQUENCE_LENGTH = 1000\n",
        "  EMBED_SIZE = 300\n",
        "  \n",
        "  # create the model\n",
        "  global model\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=MAX_SEQUENCE_LENGTH))\n",
        "  model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "\n",
        "  # model.add(Dense(1, activation='sigmoid'))\n",
        "  model.add(Dense(5, activation='sigmoid'))\n",
        "  # model.add(Flatten())\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------\n",
        "accuracy calculation function\n",
        "\n",
        "عمل تابع لحساب الدقة"
      ],
      "metadata": {
        "id": "CBw7IYEkWWD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm-pc34lkk25"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "def calculate_results(y_true, y_pred):\n",
        "  # Calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                  \"precision\": model_precision,\n",
        "                  \"recall\": model_recall,\n",
        "                  \"f1\": model_f1}\n",
        "  return model_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------\n",
        "**3- Feature Extraction:**  Word Embedding\n",
        "\n",
        "استخراج الميزات(تحويل النص إلى أرقام)\n",
        "\n",
        "\n",
        "**4- classification:** CNN\n",
        "\n",
        "تصنيف البيانات\n",
        "\n",
        "with Cross validation\n",
        "\n",
        "لتوزيع عينات الاختبار والتأكد من عدم وجود\n",
        "\n",
        "Overfitting"
      ],
      "metadata": {
        "id": "Lsfshy_jWzs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# build train and test datasets\n",
        "reviews = dataset['text'].values\n",
        "sentiments = dataset['targe'].values\n",
        "#####\n",
        "strtfdKFold = StratifiedKFold(n_splits=4)#5\n",
        "kfold = strtfdKFold.split(reviews, sentiments)\n",
        "result_CNNClassifier=[]\n",
        "#\n",
        "for k, (train_index, test_index) in enumerate(kfold):\n",
        "    # if(os.path.exists(\"drive/MyDrive/mining/CNN.net\")):\n",
        "    #   print(\"load net\")\n",
        "    #   model = keras.models.load_model('drive/MyDrive/mining/CNN.net') \n",
        "\n",
        "    X_train_, X_test_ = reviews[train_index], reviews[test_index]\n",
        "    y_train_, y_test_ = sentiments[train_index], sentiments[test_index]\n",
        "\n",
        "    X_train, X_test ,y_train,y_test=Tokenizer_UNK(X_train_,X_test_,y_train_,y_test_)\n",
        "    # set_model()\n",
        "\n",
        "    # Fit the model\n",
        "    model=set_model()\n",
        "    EPOCHS=2  \n",
        "    BATCH_SIZE=128\n",
        "    model.fit(X_train, y_train, validation_split=0.1, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
        "    y_pred=(model.predict(X_test))\n",
        "    y_classes_pre = [np.argmax(y, axis=None, out=None) for y in y_pred]\n",
        "    y_classes_test = [np.argmax(y, axis=None, out=None) for y in y_test]\n",
        "    result_CNNClassifier.append(calculate_results(y_classes_test, y_classes_pre))\n",
        "    # model.save('drive/MyDrive/mining/CNN.net')\n",
        "    # print(\"save net\")\n",
        "\n",
        "result_CNNClassifier"
      ],
      "metadata": {
        "id": "e_0JxFY4oRce",
        "outputId": "081ea326-7316-408f-e609-8ffe19c1374b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size=175892\n",
            "Number of Documents=17361\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1000, 300)         52767600  \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 1000, 128)         153728    \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 500, 128)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 500, 64)           32832     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 250, 64)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 250, 32)           8224      \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 125, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4000)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               1024256   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53,987,925\n",
            "Trainable params: 53,987,925\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "92/92 [==============================] - 298s 3s/step - loss: 0.3264 - accuracy: 0.5797 - val_loss: 0.1233 - val_accuracy: 0.8994\n",
            "Epoch 2/2\n",
            "92/92 [==============================] - 298s 3s/step - loss: 0.0462 - accuracy: 0.9669 - val_loss: 0.0571 - val_accuracy: 0.9516\n",
            "Vocabulary size=175892\n",
            "Number of Documents=17361\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 1000, 300)         52767600  \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 1000, 128)         153728    \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 500, 128)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 500, 64)           32832     \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPooling  (None, 250, 64)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 250, 32)           8224      \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPooling  (None, 125, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4000)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               1024256   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53,987,925\n",
            "Trainable params: 53,987,925\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "92/92 [==============================] - 303s 3s/step - loss: 0.3147 - accuracy: 0.6002 - val_loss: 0.0986 - val_accuracy: 0.9256\n",
            "Epoch 2/2\n",
            "92/92 [==============================] - 298s 3s/step - loss: 0.0462 - accuracy: 0.9678 - val_loss: 0.0626 - val_accuracy: 0.9486\n",
            "Vocabulary size=175892\n",
            "Number of Documents=17361\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 1000, 300)         52767600  \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 1000, 128)         153728    \n",
            "                                                                 \n",
            " max_pooling1d_6 (MaxPooling  (None, 500, 128)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 500, 64)           32832     \n",
            "                                                                 \n",
            " max_pooling1d_7 (MaxPooling  (None, 250, 64)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 250, 32)           8224      \n",
            "                                                                 \n",
            " max_pooling1d_8 (MaxPooling  (None, 125, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 4000)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               1024256   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53,987,925\n",
            "Trainable params: 53,987,925\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "92/92 [==============================] - 302s 3s/step - loss: 0.3359 - accuracy: 0.5531 - val_loss: 0.1009 - val_accuracy: 0.9194\n",
            "Epoch 2/2\n",
            "92/92 [==============================] - 300s 3s/step - loss: 0.0501 - accuracy: 0.9647 - val_loss: 0.0605 - val_accuracy: 0.9501\n",
            "Vocabulary size=175892\n",
            "Number of Documents=17361\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 1000, 300)         52767600  \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 1000, 128)         153728    \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPooling  (None, 500, 128)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 500, 64)           32832     \n",
            "                                                                 \n",
            " max_pooling1d_10 (MaxPoolin  (None, 250, 64)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 250, 32)           8224      \n",
            "                                                                 \n",
            " max_pooling1d_11 (MaxPoolin  (None, 125, 32)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 4000)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               1024256   \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53,987,925\n",
            "Trainable params: 53,987,925\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "92/92 [==============================] - 301s 3s/step - loss: 0.3229 - accuracy: 0.5886 - val_loss: 0.1137 - val_accuracy: 0.9064\n",
            "Epoch 2/2\n",
            "92/92 [==============================] - 299s 3s/step - loss: 0.0538 - accuracy: 0.9594 - val_loss: 0.0790 - val_accuracy: 0.9340\n",
            "CPU times: user 1h 18min 34s, sys: 1min, total: 1h 19min 34s\n",
            "Wall time: 43min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "حساب متوسط النتائج للـ5 مراحل من\n",
        "\n",
        " cross validation "
      ],
      "metadata": {
        "id": "In5dWzCIXIyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy=0\n",
        "f1=0\n",
        "precision=0\n",
        "recall=0\n",
        "for i in result_CNNClassifier:\n",
        "  accuracy+=(i['accuracy'])\n",
        "  f1+=(i['f1'])\n",
        "  precision+=(i['precision'])\n",
        "  recall+=(i['recall'])\n",
        "\n",
        "accuracy=accuracy/len(result_CNNClassifier)\n",
        "f1=f1/len(result_CNNClassifier)\n",
        "precision=precision/len(result_CNNClassifier)\n",
        "recall=recall/len(result_CNNClassifier)\n",
        "# (result_DecisionTreeClassifier)\n",
        "results_CNNClassifier=[{'accuracy':accuracy,'f1':f1,'precision':precision,'recall':recall}]\n",
        "results_CNNClassifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDPaXpSn0p-S",
        "outputId": "4b1f978f-5a8d-4f2f-8870-4078b6a6c610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'accuracy': 95.05209411494941,\n",
              "  'f1': 0.9507144665712016,\n",
              "  'precision': 0.9517038129325414,\n",
              "  'recall': 0.950520941149494}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "-----\n",
        "---------\n",
        "**5- Show Results**\n",
        "\n",
        "النتائج\n",
        "\n",
        "نتائج التدريب لكل قاعدة بيانات"
      ],
      "metadata": {
        "id": "gXWU0K4mXLVq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e-5UX4r2kr7e",
        "outputId": "331785c6-9367-4029-b7d8-06ed4c2153d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNClassifier results in dataset 1:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'accuracy': 95.19608342701729,\n",
              "  'f1': 0.9520984811390116,\n",
              "  'precision': 0.952799253715478,\n",
              "  'recall': 0.9519608342701729}]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "print('CNNClassifier results in dataset 1:')\n",
        "results_CNNClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yU3FWF6a0_9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ac5ce3-b2d1-4b5c-b085-4fad6d8eb0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNClassifier results in dataset 2:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'accuracy': 95.57787784320516,\n",
              "  'f1': 0.9558535748300416,\n",
              "  'precision': 0.9565019386666695,\n",
              "  'recall': 0.9557787784320517}]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "print('CNNClassifier results in dataset 2:')\n",
        "results_CNNClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OvnBdsMv1AHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d22ea8-203c-457d-8f62-8e4191151801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNClassifier results in dataset 3:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'accuracy': 94.5945934848568,\n",
              "  'f1': 0.9460234167421608,\n",
              "  'precision': 0.9465212822533673,\n",
              "  'recall': 0.9459459348485679}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "print('CNNClassifier results in dataset 3:')\n",
        "results_CNNClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P4AvEaX51ASP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20574d39-9379-4a0d-beff-40a9e9758739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNClassifier results in dataset 4:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'accuracy': 94.78257724360446,\n",
              "  'f1': 0.9477387119920051,\n",
              "  'precision': 0.9479345864832764,\n",
              "  'recall': 0.9478257724360446}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "print('CNNClassifier results in dataset 4:')\n",
        "results_CNNClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Final_Result=[\n",
        "  {'accuracy': 95.19608342701729,\n",
        "  'f1': 0.9520984811390116,\n",
        "  'precision': 0.952799253715478,\n",
        "  'recall': 0.9519608342701729}\n",
        "  ,\n",
        "  {'accuracy': 95.57787784320516,\n",
        "  'f1': 0.9558535748300416,\n",
        "  'precision': 0.9565019386666695,\n",
        "  'recall': 0.9557787784320517}\n",
        "  ,\n",
        "  {'accuracy': 94.5945934848568,\n",
        "  'f1': 0.9460234167421608,\n",
        "  'precision': 0.9465212822533673,\n",
        "  'recall': 0.9459459348485679}\n",
        "  ,\n",
        " {'accuracy': 94.78257724360446,\n",
        "  'f1': 0.9477387119920051,\n",
        "  'precision': 0.9479345864832764,\n",
        "  'recall': 0.9478257724360446}\n",
        "  ]\n",
        "\n",
        "accuracy=0\n",
        "f1=0\n",
        "precision=0\n",
        "recall=0\n",
        "for i in Final_Result:\n",
        "  accuracy+=(i['accuracy'])\n",
        "  f1+=(i['f1'])\n",
        "  precision+=(i['precision'])\n",
        "  recall+=(i['recall'])\n",
        "\n",
        "accuracy=accuracy/len(Final_Result)\n",
        "f1=f1/len(Final_Result)\n",
        "precision=precision/len(Final_Result)\n",
        "recall=recall/len(Final_Result)\n",
        "# (result_DecisionTreeClassifier)\n",
        "Final_Result=[{'accuracy':accuracy,'f1':f1,'precision':precision,'recall':recall}]\n",
        "\n",
        "print('Final_Result:')\n",
        "print(Final_Result)"
      ],
      "metadata": {
        "id": "-lB0leWu1IwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f16600-3be5-4763-f9f5-c7922c142b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final_Result:\n",
            "[{'accuracy': 95.03778299967092, 'f1': 0.9504285461758049, 'precision': 0.9509392652796977, 'recall': 0.9503778299967094}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_qyQzgTNUlO"
      },
      "source": [
        "---------------\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CNN_Embedding.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}